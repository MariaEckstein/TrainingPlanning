\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology \\
University of California, Berkeley \\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Department of Psychology \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
Department of Psychology\\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Abstract. 
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{This research was supported by fellowship from the German Academic Exchange Service (DAAD) and by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment.}  


\section*{TDs:}
\begin{itemize}
	\item Simulate data using participants' parameter values -> recover
	\item Do some more research on mb / mf studies
	\item Fill in missing stuff in training task table (brain areas \& screen shots)
	\item Abstract
\end{itemize}

\section*{Updates:}
\begin{itemize}
	\item Model with lowest AIC and BIC: lambda == 1
\end{itemize}

\startmain % to start the main 1-4 pages of the submission.

\section{Introduction}

Humans make many decisions every day: Which restaurant to pick for dinner, which book to select for reading, where to go for the weekend. What strategies do humans use to make such decisions? In the behaviorist tradition of psychology (Skinner, Watson, Pavlov), researchers have contended that all observed behavior, ranging from button pressing in rats to language production in humans, can be explained conditioned responses elicited by specific stimuli (Skinner, 1977). In this view, all behavior is the product of shaping, in which the reward characteristics of the environment lead to the formation of stable stimulus-response associations. According to this view, we pick a restaurant where we have had good experiences so far. Cognitive psychologists, on the other hand, contend that humans and animals possess internal representations of their environment and are able to reason about causal relationships and contingencies (Tolman, 1948). In the cognitivist view, agents choose specific actions based on mental simulations of their likely consequences. In this view, we pick a restaurant that leads to the best result in our mental simulation, based on our world knowledge.

These two opposing characterizations of decision making, behaviorist and cognitivist, are mirrored in one of the three big frameworks of machine learning, reinforcement learning (Sutton \& Barto, 2012). In reinforcement learning, sequential tasks are characterized as Markov Decision Problems (MDPs), in which agents transfer between states by employing actions, and receive rewards in certain states. An agent's goal is to maximize rewards in the long run, i.e., to learn an optimal mapping between states and action probabilities, so as to choose an optimal path through the state space. Model-free agents determine the values of actions in each state by averaging over their past reward history, similar to the behaviorist framework in psychology. Model-based agents, on the other hand, use an internal model of the environment, including transition probabilities between states and reward probabilities, to simulate action outcomes, similar to the cognitivist tradition.

Bringing psychology and machine learning together, past research has shown that human and animal behavior can be modeled through reinforcement learning. A major finding concerns the striking similarity between activity in the dopamine system, the brain's "reward system" (Wise, 1982), and the occurrence of reward prediction errors as specified in reinforcement learning (Schultz, 1997). More recently, researchers have sought to differentiate model-based and model-free decision strategies in humans and to determine whether different brain regions underlie each. Model-free decision making, fast and low in cognitive demand, increases when time and/or cognitive resources are sparse, for example during stress (Schwabe \& Wolf, 2011) or when multi-tasking (Otto, Gershman, Markman, \& Daw, 2013). The brain regions involved in model-free decision making include subcortical regions and the ventral striatum (Dolan \& Dayan, 2013). On the other hand, no situations have yet been identified that increase model-based decision making, although some brain regions underlying these processes have been identified (frontal cortex and dorsal striatum; Dolan \& Dayan, 2013). The only study that has shown an increase in model-based decision making was a pharmacological manipulation of dopamine levels (Wunderlich, Smittenaar, \& Dolan, 2012). 

In the current study, we therefore sought to investigate whether a cognitive intervention would increase model-based decision making in human participants. We hypothesized that decision strategies are influenced by prior cognitive activities. Forward-planning and mental simulation should influence model-based decisions, whereas habitual, reward-driven behavior should influence model-free decisions. This influence could go either way, fostering (cognitive enhancement hypothesis; CITE Jaeggi?) or depressing (cognitive depletion hypothesis; CITE) the decision strategy. An increase in model-based decision making after model-based activities would support the cognitive enhancement hypothesis; an increase of model-based decision making after model-free activities would support the cognitive depletion hypothesis.

In order to rule out alternative explanations of such changes, we also included a control group who received cognitive tasks that were not related to (model-based or model-free) decision making. This group was supposed to reveal how decision strategies naturally evolve over time in humans tested multiple times on the same task. We pre-registered these hypotheses on the Open Science Framework prior to data collection (www.osf.io/).

\begin{figure}
	\includegraphics[width=\linewidth]{TrainingTasks.png}
	\label{TrainingTasks}
\end{figure}

\section{Methods and Results}
\subsection{Study Design}

116 participants took part in a two-session experiment (sessions 2 days apart). We assessed participants' decision strategies at the beginning and end of each session, i.e., four times in total. Participants were randomly assigned to one of five groups: control, MB-MB, MF-MF, MB-MF, or MF-MB. The control group received the control tasks in both sessions, MB-MB received model-based tasks in both sessions, MF-MF received model-free in both sessions, MB-MF received model-based in the first and model-free in the second session and MF-MB in reversed order. The tasks are shown in Table 1. We present the effects of the first session here, collapsing groups with the same tasks, i.e., combining MB-MB and MB-MF and combining MF-MF and MF-MB.

\begin{figure}
	\includegraphics[width=\linewidth]{TwoStepab.png}
	\caption{Two-step task (A) and results (B). (A) In the first state $s_{1}$ participants decide between two actions. One action is followed by state $s_{2a}$ most of the time ("common transition"), the other one by state $s_{2b}$. In state $s_{2}$ participants again decide between two actions and receive a reward with probability determined by $a_{2}$. Reward probabilities change over time to ensure continued learning. (B) Participants' probability of repeating last trial's $a_{1}$ ("s1 stay probability"), as a function of reward and transition in the previous trial.}
	\label{TwoStep}
\end{figure}

We assessed decision strategies using Daw et al.'s (cite) 2-step task. The task involves two binary decisions, whereby the second set of choices is statistically contingent on participants' first decision (see Figure \ref{TwoStep}). Model-based and model-free strategies lead to different decisions in this task. A model-free agent learns the values of a state-one action $a_{1}$ by averaging the rewards obtained in all trials, in which $a_{1}$ was selected. The model-free agent does therefore not discriminate between common and rare transitions. A model-based agent, on the other hand, selects $a_{1}$ based on its knowledge about the structure of the task, reasoning which $a_{1}$ most likely leads to the $s_{2}$ with the most-rewarded action. A model-based agent therefore assigns values differently after common and rare state transition.

\subsection{Behavioral Switch-Stay Analysis}
Participants' decision making strategies in the 2-step task can be analyzed in two ways. Behavioral analyses assess the influence of past rewards on participants' subsequent decisions (model-free characteristic) and they asses whether participants differentiate between common and rare transitions (model-based characteristic). RL modeling tests for model-based and model-free decisions explicitly by fitting RL models to the human data and estimating the weight of model-based and model-free decision making.

Before the analyses, we cleaned the data using standard procedures. One dataset of 331 was excluded because only one button was pressed throughout the task. 13 datasets were removed because the same $a_{1}$ was selected in more than 95\% of trials. All trials were removed in which participants responded faster than 100 msec (average 8\% per participant) and 14 complete datasets were excluded because more than 75\% of the trials had been removed. The resulting data contained 303 datasets from 114 participants.

We first analyzed the 2-step task behaviorally (see Figure \ref{TwoStep}B). Participants tended to repeat rewarded actions more often than unrewarded actions, a sign of model-free behavior. This effect was statistically significant in all runs in the MF and MB groups, but in no run in the control group, as revealed by main effects of reward in the previous trial on staying in logistic mixed-effects regression models, INSERT STATS. Besides this model-free component, participants also showed model-based decision making in that they differentiated between common and rare transitions. This was reflected in the interaction between reward and transition in runs 2 and 3 in the control group, run 2 in the MB group, and runs 2 and 3 in the MF group (although the pattern was reversed in run 3, so should not be taken as support for a model-based strategy), STATS. We then tested for differences between groups, using interaction contrasts. We found that the model-free component did not differ between groups in any run, revealed by non-significant interactions between reward and group, STATS. The model-based component, on the other hand, differed in run 3, as revealed by a significant interaction between reward, interaction, and group, STATS.

Taken together, the control group showed a combination of model-based and model-free behavior in runs 2 and 3, whereas the MB and MF groups showed model-free behavior in all runs and only had an additional model-based component in the second run. These analyses were supported by regression models that modeled the influence of outcomes further in the past and by logistic regression models on individual participants (not shown). The results were also similar when 

\subsection{RL Modeling Analysis}
We next assessed participants' decision process using RL modeling. We first specified a hybrid model, in which agents determine action values by combining model-based and model-free value estimates. We then fit this model to each participant's actions by selecting parameter values that maximized the likelihood of the actions under the model. The parameter $w$, which determined the weight of model-based and model-free value estimates, was used to assess participants' decision strategy.

Our model was similar to the ones previously described for the 2-step task (CITE). In specific, agents updated action values $Q$ for actions $a_{2}$ available in the second stage by observing the trial outcome (reward $R = 1$ or $R = 0$), as follows:

\begin{equation}
Q(s_{2}, a_{2}) = Q(s_{2}, a_{2}) + \alpha_{2} \cdot RPE,
\end{equation}

whereby the reward prediction error $RPE = R - Q(s_{2}, a_{2})$ and $\alpha_{2}$ is the agent's learning rate for the second state.

The update of first-stage action values $Q(s_{1}, a_{1})$ differed between model-based and model-free agents. Model-free agents used the outcome of $a_{1}$ to update $a_{1}$'s value, i.e., the value of the action chosen in $s_{2}$ and the trial's ultimate outcome ($R = 1$ or $R = 0$).

\begin{equation}
Q_{mf}(s_{1}, a_{1}) = Q_{mf}(s_{1}, a_{1}) + \alpha_{1} \cdot (VPE + \lambda \cdot RPE),
\end{equation}

whereby the value prediction error $VPE = Q_{mf}(s_{2}, a_{2}) - Q_{mf}(s_{1}, a_{1})$ and The weight of the RPE for first-stage updating was determined by $ \lambda $, a temporal discounting factor. The model with $\lambda = 1$ was the best-fitting model for our participants, suggesting that participants weighted RPEs equally strong as VPE when updating action values $Q_{mf}(s_{1}, a_{1})$.

Model-based agents determined the values of first-stage actions $a_{1}$ based on their internal predictive model, taking into account the transition probability $p(s_{2}, a_{1}, s_{1})$ between state $s_{1}$ and state $s_{2}$, when choosing action $a_{1}$:

\begin{equation}
Q_{mb}(s_{1}, a_{1}) = \sum_{s_{2}} p(s_{2}, a_{1}, s_{1}) \cdot max(Q(s_{2}, a_{2}))
\end{equation}

Agents combined model-based and model-free value estimates using a weighted average, $Q_{hyb}(s_{1}, a_{1}) = (1 - w) \cdot Q_{mf}(s_{1}, a_{1}) + w \cdot Q_{mb}(s_{1}, a_{1})$, whereby the parameter $w$ determined the weight of model-based versus model-free values. Agents selected actions $a$ according to a softmax decision rule, which took into account the actions' values $Q(s, a)$, but also whether the same action was taken in the previous trial (choice perseverance $p$) and whether the same key was pressed in the previous trial (key perseverance $k$). The inclusion of key perseverance is the only difference to previous models and improved the model fit significantly. Individuals' softmax temperature with respect to action values $Q(s, a)$ was determined by parameters $\beta_{1}$ and $\beta_{2}$ for the fist and second state. We validated our model by simulating agents with different parameter values and subsequently recovering these parameters (data not shown). We also confirmed that reduced models, with certain parameters fixed, were best recovered by models with the same free parameters, rather than different models. 

\begin{figure}
	\includegraphics[width=0.5\linewidth]{gg_params_s1.png}
	\caption{Results of the RL analysis. Means and standard errors of model parameters fitted to individuals' datasets.}
	\label{Params}
\end{figure}

We then aimed to characterize human performance in terms of the model parameters---learning rates $\alpha_{1}$ and $alpha_{2}$, decision temperature $beta_{1}$ and $beta_{2}$, key perseverance $k$, choice perseverance $p$, and crucially the balance between model-based and model-free decision making $w$. When fitting the model to human data, the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) were lowest for a reduced model with 7 free parameters, fixing future discounting $\lambda$ at 1, as revealed by sign rank tests, STATS. Matlab's fmincon function was used with 30 different start values to find the best-fitting parameters for each dataset. The results (Figure \ref{Params}) show that $w$ increased slightly in run 2 in the control and MF group, in accordance with the behavioral analyses. Nevertheless, this effect was not statistically significant in a regression model testing for effects of group and run on $w$.

\section{Conclusion}
We present the results of a study aimed at investigating factors that influence human decision making. In specific, we aimed to test whether the use of model-based versus model-free strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would influence model-based decision strategies, whereas habitual, reward-driven, stimulus-response behavior would influence model-free decision making. We found a slight, despite non-significant increase in model-based decision making after cognitive tasks unrelated to model-based decision making, supporting the cognitive depletion hypothesis, which states that cognitive activities are depleted upon use. The biggest differences seemed to arise between the control group and the two decision making groups MB and MF (switch-stay analysis), where participants unexpectedly favored common over rare transitions, an effect unrelated to model-based or model-free decision making. This suggests that the results of the RL modeling should be taken with a grain of salt.

One reason for this negative results might be that the carry-over from previous cognitive activities did not last long enough to determine the strategy in the whole 2-step task (approximately 20 minutes). Lastly, there were a differences between the design of the pilot study, which revealed strong effects of short-term cognitive depletion and long-term cognitive enhancement. Namely, participants only did the 2-step task twice in the pilot study, at the end of each session, and the training of the 2-step task was interleaved with the training tasks. In the current study, the 2-step task was the first task of all, before the training. The effect might be weaker because of that. Maybe it only works when participants have not seen the 2-step before the cognitive intervention.

Concluding, research into the manipulation of model-based and model-free decision strategies is of great relevance. Model-free decisions are often short-sighted and suboptimal in the long term (eating the chocolate cake). Model-based decisions (preparing for an exam) require more effort but often to better long-term outcomes when knowledge about environmental contingencies are relevant. Many vulnerable subjects, including persons with ADHD, depression, schizophrenia, Parkinson's disease, obesity, impulsitiy, eating disorders, or addiction would benefit from a cognitive intervention that facilitates model-based decision making and creates some ease in selecting and switching between both strategies, according to the context.

\section{References}


\end{document}
