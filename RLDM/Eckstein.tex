\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}
\usepackage{amsmath}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology \\
University of California, Berkeley \\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Department of Psychology \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
Department of Psychology\\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Abstract. 
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{This research was supported by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment as well as by a fellowship from the German Academic Exchange Service (DAAD).}  


\section*{TDs:}
\begin{itemize}
	\item Do some more research on mb / mf studies
	\item Abstract
	\item Simulate data using participants' parameter values -> recover
\end{itemize}

\startmain

\section{Introduction}

Animals as well as humans make habitual decisions, for example when following the same path that we take every day, following a fixed sequence of actions and using landmarks on the way as triggers for subsequent actions. But animals and humans also make goal-directed decisions, for example when planning a route in our minds, based on our knowledge about the area, landmarks, and streets, flexibly combining pieces of knowledge into a route that we might have never taken before. These two different modes of decision making, habitual and goal-directed, have long been differentiated in psychology, and form the basis of two very different schools of thoughts, namely Behaviorism (Watson; Skinner; Pavlov) and cognitivism (Tolman; Chomsky; Anderson; Miller). A similar differentiation is evident in a very different field of decision making, namely reinforcement learning in machine learning. Here, the distinction is between model-based (goal-directed) and model-free (habitual) agents, using a model of the environment to simulate possible actions and outcomes on the one hand, and determining which action has the largest expected reward based on previous experience, without the use of an internal model, on the other. 

Recently, reinforcement learning algorithms have been used in psychology to characterize the decision process more precisely and to identify the brain regions involved. A major finding concerns the striking similarity between activity in the dopamine system, the brain's "reward system" (Wise, 1982), and the occurrence of reward prediction errors as specified in reinforcement learning (Schultz, 1997). In terms of model-based and model-free decision strategies, studies have shown that model-free decision making, fast and low in cognitive demand, increases when time and/or cognitive resources are sparse, for example during stress (Schwabe \& Wolf, 2011) or when multi-tasking (Otto, Gershman, Markman, \& Daw, 2013). Brain regions involved in model-free decision making include subcortical regions and the ventral striatum (Dolan \& Dayan, 2013). On the other hand, no situations have yet been identified that increase model-based decision making, although the brain network underlying these processes has been identified (frontal cortex and dorsal striatum; Dolan \& Dayan, 2013). The only study that has shown an increase in model-based decision making was a pharmacological manipulation of dopamine levels (Wunderlich, Smittenaar, \& Dolan, 2012). 

In the current study, we therefore sought to investigate whether a cognitive intervention could increase model-based decision making. We hypothesized that decision strategies are influenced by prior cognitive activities. Forward-planning and mental simulation should influence model-based decisions, whereas habitual, reward-driven behavior should influence model-free decisions. This influence could go either way, fostering (cognitive enhancement hypothesis; CITE Jaeggi?) or depressing (cognitive depletion hypothesis; CITE) the subsequent decision strategy. An increase in model-based decision making after model-based activities would support the cognitive enhancement hypothesis; an increase of model-based decision making after model-free activities would support the cognitive depletion hypothesis. We pre-registered these hypotheses on the Open Science Framework prior to data collection (www.osf.io/).

\section{Methods and Results}
\subsection{Study Design and Tasks}

116 participants took part in a two-session experiment, as shown in Figure \ref{TwoStep}A. Participants were randomly assigned to one of three groups: model-based intervention (MB), model-free intervention (MF), and control. Participants in the model-based group engaged in two tasks that were closely related to model-based decision making, a planning-intensive version of the Tower of London task (CITE), and a rule-based category learning task (Ashby \& Maddox). The Tower of London task is model-based because it requires participants to use an internal model of the task to simulate potential moves and their outcomes, and to select the best sequence of moves. The rule-based category task has been shown to rely on model-based mechanisms, for example in that learning is unaffected by delayed feedback (Ashby and Maddox). Indeed, both Tower of London and the rule-based category learning task engage brain regions similar to model-based decision making (Beauchamp, Dagher, Aston \& Doyon, 2003; Ashby and Maddox).

Participants in the model-free group engaged in tasks targeted at model-free processes, a habitual reward-based task (Tricomi) and an information-integration category learning task (Ashby and Maddox). The habitual reward-based task is a classical example of model-free, habitual learning, where participants were rewarded when they pressed the correct button in response to simple visual stimuli. The information-integration category task relies on model-free processes, as is evident in that performance is negatively affected by interventions that disrupt model-free learning, such as lack of immediate feedback. Again, both tasks have been shown to engage brain regions underlying model-free decisions. Finally, the control tasks were unrelated to (model-based or model-free) decision making. The number comparison task engages the approximate number sense and X and Y brain regions (Piazza, 2010). The orientation discrimination task involves perceptual learning and early visual processing and is associated with activity in the retinotopic region of V1 corresponding to the location of stimulus presentation (Kim, Ling, Watanabe, 2015). 

\begin{figure}
	\includegraphics[width=\linewidth]{TwoStepab2.png}
	\caption{Experimental procedure (A) and 2-step task (B). (A) In session 1, participants' decision strategies were assessed using the 2-step task, then of the three cognitive interventions followed, then participants did the 2-step task again. Two days later, participants came in for session 2 and did a third assessment of the 2-step task. (B) The 2-step task has two subsequent states, $s_{1}$ and $s_{2}$. In $s_{1}$, participants decide between two actions $a_{1}$ (blue vs pink). One action (blue) is followed by state $s_{2a}$ most of the time ("common transition"), the other one (pink) by state $s_{2b}$. In state $s_{2}$ participants decide between two actions and receive a reward with probability $P(R|a_{2})$ determined by $a_{2}$. Reward probabilities change over time to ensure continued learning.}
	\label{TwoStep}
\end{figure}

Participants' decision strategies were assessed using Daw et al.'s (cite) 2-step task. This task involves two binary decisions, whereby the second set of choices is statistically contingent on the first decision (see Figure \ref{TwoStep}B). This task is designed so that model-based and model-free strategies lead to different choices. A model-free agent learns the values of a state-one action $a_{1}$ by averaging the rewards obtained in all trials, in which $a_{1}$ was selected. Future actions are therefore mainly driven by past reward. A model-based agent, on the other hand, selects $a_{1}$ based on its knowledge of the task structure, taking into account the transition probabilities between $s_{1}$ and $s_{2}$ when reasoning which $a_{1}$ to take. Future actions therefore depend on both past rewards and the transition probability of rewarded trials.

\subsection{Behavioral Switch-Stay Analysis}
The 2-step task can be analyzed in two ways. Behavioral analyses assess the influence of past rewards on participants' subsequent decisions (model-free characteristic) and they asses whether participants differentiate between common and rare transitions (model-based characteristic). RL modeling tests for model-based and model-free decisions explicitly by fitting RL models to the human data and estimating the weight of model-based and model-free decision making.

Before the analyses, we cleaned the data using standard procedures (CITE). The resulting data consisted of 303 datasets from 114 participants (110 in run 1---44 MB, 43 MF, 21 control; 103 in run 2---43 MB, 41 MF, 19 control; and 90 in run 3---36 MB, 35 MF, 19 control). We first analyzed the 2-step task behaviorally (see Figure \ref{Results}A). Participants tended to repeat rewarded actions more often than unrewarded actions, a sign of model-free behavior. This effect was statistically significant in all runs in the MF and MB groups, but not in the control group, as revealed by main effects of reward in the previous trial on staying in logistic mixed-effects regression models including the factors choice repetition and key repetition, INSERT STATS. Besides this model-free component, participants also showed model-based decision making in that they differentiated between common and rare transitions. This was reflected in the interaction between reward and transition in runs 2 and 3 in the control group, run 2 in the MB group, and in runs 2 and 3 in the MF group (although the pattern was reversed here in run 3, so should not be taken as support for a model-based strategy), STATS. We then tested for differences between groups, using interaction contrasts. We found that the model-free component did not differ between groups in any run, revealed by non-significant interactions between reward and group, STATS. The model-based component, on the other hand, differed in run 3, as revealed by a significant interaction between reward, interaction, and group, STATS.

Taken together, the control group showed predominantly model-based behavior in runs 2 and 3. The MB and MF groups, on the other hand, showed model-free characteristics in all runs and additional model-based components in run 2 only. These analyses were also supported by regression models on the influence of outcomes further in the past and by logistic regression models on individual participants (not shown). The results were also similar when only including data from participants who performed well on the training tasks.

\subsection{RL Modeling Analysis}
We next assessed participants' decision strategies using RL modeling. We specified a hybrid model, in which agents determine action values by combining model-based and model-free value estimates. We then fit this model to each participant's actions by selecting parameter values that maximized the likelihood of the actions under the model. The parameter $w$, which determined the weight of model-based and model-free value estimates, was used to assess participants' decision strategy.

Our model was similar to the ones previously described for the 2-step task (CITE). In specific, agents updated action values $Q$ for actions $a_{2}$ available in the second stage by observing the trial outcome (reward $R = 1$ or $R = 0$), as follows:

\begin{equation}
Q(s_{2}, a_{2}) = Q(s_{2}, a_{2}) + \alpha_{2} \cdot RPE,
\end{equation}

whereby the reward prediction error $RPE = R - Q(s_{2}, a_{2})$ and $\alpha_{2}$ is the agent's learning rate in the second state.

The update of first-stage action values $Q(s_{1}, a_{1})$ differed between model-based and model-free agents. Model-free agents used the outcome of $a_{1}$ to update $Q(s_{1}, a_{1})$. The outcome of $a_{1}$ consists in the value of the action chosen in $s_{2}$ and the trial's ultimate outcome ($R = 1$ or $R = 0$).

\begin{equation}
Q_{mf}(s_{1}, a_{1}) = Q_{mf}(s_{1}, a_{1}) + \alpha_{1} \cdot (VPE + \lambda \cdot RPE),
\end{equation}

whereby the value prediction error $VPE = Q_{mf}(s_{2}, a_{2}) - Q_{mf}(s_{1}, a_{1})$. The weight of the RPE for first-stage updating was determined by $ \lambda $, a temporal discounting factor. The model with $\lambda = 1$ was the best-fitting model for our participants, suggesting that participants weighted RPEs equally strong as VPE when updating action values $Q_{mf}(s_{1}, a_{1})$.

Model-based agents determined $Q(s_{1}, a_{1})$ based on an internal predictive model, taking into account the transition probability $p(s_{2}, a_{1}, s_{1})$ between state $s_{1}$ and state $s_{2}$, when choosing action $a_{1}$:

\begin{equation}
Q_{mb}(s_{1}, a_{1}) = \sum_{s_{2}} p(s_{2}, a_{1}, s_{1}) \cdot max(Q(s_{2}, a_{2}))
\end{equation}

Agents combined model-based and model-free value estimates using a weighted average, $Q_{hyb}(s_{1}, a_{1}) = (1 - w) \cdot Q_{mf}(s_{1}, a_{1}) + w \cdot Q_{mb}(s_{1}, a_{1})$, whereby the parameter $w$ determined the weight of model-based versus model-free values. Agents selected actions $a$ according to a softmax decision rule, which took into account the actions' values $Q(s, a)$, but also whether the same action had been taken in the previous trial (choice perseverance $p$) and whether the same key had been pressed in the previous trial (key perseverance $k$). The inclusion of key perseverance is an extension of previous models and improved the model fit significantly. Individuals' softmax temperature with respect to action values $Q(s, a)$ was determined by parameters $\beta_{1}$ and $\beta_{2}$ for the fist and second state. We validated our model by simulating agents with different parameter values and subsequently recovering these parameters (data not shown). We also confirmed in simulations that reduced models, with certain parameters fixed, were best recovered by models with the same free parameters, rather than different models. 

\begin{figure}
	\includegraphics[width=\linewidth]{Results.png}
	\caption{Results of the 2-step analyses. (A) Participants' probability of repeating action last $a_{1}$ of the last trial ("s1 stay probability"), as a function of reward and transition in the previous trial. (B) Means and standard errors of model parameters fit to individual datasets. a1: $\alpha_{1}$, a1: $\alpha_{2}$, b1: $beta_{1}$, b2: $\beta_{2}$.}
	\label{Results}
\end{figure}

We then aimed to characterize human performance in terms of the model parameters---learning rates $\alpha_{1}$ and $\alpha_{2}$, decision temperatures $\beta_{1}$ and $\beta_{2}$, key perseverance $k$, choice perseverance $p$, and crucially the balance between model-based and model-free decision making $w$. When fitting the model to human data, both Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) were lowest for a reduced model with 7 free parameters, fixing future discounting $\lambda$ at 1, as revealed by sign rank tests, STATS. Matlab's fmincon function was used with 30 different start values to find the best-fitting parameters for each dataset. The results (Figure \ref{Results}B) show that $w$ increased in run 2 in the MF group, in accordance with the behavioral analyses. Nevertheless, this effect was not statistically significant in a mixed-effects regression model testing for effects of group and run on $w$ (SHOULD I TEST GROUPS INDIVIDUALLY INSTEAD, COMPARING RUN 1 TO RUN 2 AND RUN 1 TO RUN 3?).

\section{Conclusion}
We have presented the results of a study aimed at investigating factors that influence human decision making. In specific, we aimed to test whether the use of model-based versus model-free strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would influence model-based decision strategies, whereas habitual, reward-driven stimulus-response behavior would influence model-free decision making. We found that model-based decision making was highest in the control group, and potentially increased slightly in MB and MF in run 2. We found no long-term changes in decision strategy. (NOTE: SHOULD I TEST RUN 1 VS RUN 2 AND RUN 1 VS RUN 3 DIRECTLY? RATHER THAN PUTTING RUN AS A FACTOR INTO THE REGRESSION?).

Telling from the behavioral data (Figure \ref{Results}A), the biggest differences seemed to arise between the control group and the two decision making groups. The pattern looks quite model-based in the control group, whereas participants unexpectedly favored common over rare transitions in the MB and MF groups, an effect unrelated to model-based or model-free decision making. But this does not match up with the modeling results...?

One reason for our negative result might be that previous cognitive activities only carry over very briefly into subsequent activities, such that the strategy in the whole 2-step task, which took approximately 20 minutes, was not affected throughout. If this is the case, a shorter assessment of decision strategy might find something. In addition, there were differences between the design of the pilot study, which had initially suggested short-term cognitive depletion and long-term cognitive enhancement. Namely, participants only did the 2-step task twice in the pilot study, at the end of the training session and in the 1-week follow-up. Plus, the 2-step training was interleaved with the intervention tasks, potentially increasing carry-over effects. In the current study, the 2-step task was the first task of all, potentially leading participants to determine their personal strategy and being less flexible in changing it.

Concluding, research into the manipulation of model-based and model-free decision strategies is of great relevance. Model-free decisions are often short-sighted or suboptimal in the long term (eating the chocolate cake). Model-based decisions (preparing for an exam) require more effort but often lead to better long-term outcomes when knowledge about environmental contingencies is relevant. Many vulnerable subjects, including persons with ADHD, depression, schizophrenia, Parkinson's disease, obesity, impulsitiy, eating disorders, or addiction would benefit from a cognitive intervention that facilitates model-based decision making and creates some ease in selecting and switching between both strategies, according to the context.

\section{References}


\end{document}
