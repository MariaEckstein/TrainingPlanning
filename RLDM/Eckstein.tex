\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{TwoStep.bib}
\DeclareLanguageMapping{english}{american-apa}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology \\
University of California, Berkeley \\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Department of Psychology \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
Department of Psychology\\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
When deciding what to do, humans and animals employ (at least) two decision systems: oftentimes, we rely on habits, fixed stimulus-response associations, which have been shaped by past rewards, are computationally cheap, and enable fast responses ("model-free" decision making). But we can also---effortfully---plan our decisions, mentally simulating different courses of actions and their outcomes, and selecting the one that promises to lead to our goal ("model-based" decision making). In the current study, we aimed to increase model-based relative to model-free decision making in human participants. To this aim, we implemented a cognitive intervention, which engaged participants in forward-planning and mental simulation (model-based condition), habitual, reward-based processes (model-free condition), or unrelated processes (active control). Decision strategies were assessed using the 2-step task (Daw et al., 2011). Contrary to our expectations, the intervention did not change decision strategies and did not lead to group differences, as assessed by behavioral analyses and employing reinforcement learning models. This suggests that it is difficult to manipulate decision strategies in humans. Nevertheless, group differences emerged in that the model-based and model-free groups, compared to control, avoided actions that had previously been followed by unexpected outcomes. This implies that participants tried to avoid unexpected situations, a finding that has not yet been reported in the context of model-based and model-free decision making. More research is needed to investigate how this effect relates to the cognitive intervention. Looking ahead, more research is needed on decision strategies and their manipulation. Falling back to model-free habits under stress or time pressure often leads to negative consequences in the long run. Not only vulnerable populations would benefit from an intervention targeted at building up model-based decision making. 
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{This research was supported by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment. The research was also supported a graduate-student fellowship from the German Academic Exchange Service (DAAD).}


\section*{TDs:}
\begin{itemize}
	\item Do some more research on mb / mf studies
	\item Abstract
	\item Simulate data using participants' parameter values -> recover
\end{itemize}

\startmain

\section{Introduction}

Humans make many decisions habitually: for example, we effortlessly navigate the route we take every day, following a fixed sequences of actions, and using landmarks to trigger subsequent actions. But humans also make decisions in a goal-directed way. For example, when we plan how to get to a specific goal, we flexibly combine individual pieces into a new route, using a cognitive map or model of our environment. These two different modes of decision making, habitual and goal-directed, have long been differentiated in psychology, and form the basis of two different schools of thought, namely Behaviorism (e.g., \cite{skinner_why_1977}) and cognitivism (e.g., \cite{tolman_cognitive_1948}). A parallel differentiation between decision making strategies exists in reinforcement learning, a branch of machine learning. Here, the distinction is between model-based (MB; similar to goal-directed) and model-free (MF; similar to habitual) agents. MB agents use a model of the environment to simulate possible actions and outcomes, and then determine which actions are expected to lead to the best outcomes. In contrast, MF agents determine the value of actions by accumulating the past reward history of these actions.

Ever more often, reinforcement learning algorithms are applied in psychological research. This has led to the discovery that activity in the dopamine system, the brain's "reward system" (\cite{wise_brain_1989}), coincides with the occurrence of reward prediction errors as specified in MF reinforcement learning (\cite{schultz_neural_1997}). MB learning has been shown to rely on a distinct brain network including frontal cortex and dorsal striatum (\cite{dolan_goals_2013}). Human learning and decision making relies on both MB and MF processes (\cite{daw_model-based_2011}), and a key question is how we arbitrate between the two processes. Previous studies have shown that cognitively demanding MB decision making is less prevalent when time and/or cognitive resources are sparse, for example during stress (\cite{schwabe_stress-induced_2011}) or when multi-tasking (\cite{otto_curse_2013}). On the other hand, no situations have yet been identified that increase MB decision making. The only study that has shown an increase was a pharmacological manipulation of dopamine levels (\cite{wunderlich_dopamine_2012}). 

In the current study, we therefore sought to investigate whether a cognitive intervention could increase MB decision making. Cognitive strategies are influenced by prior cognitive activities (\cite{jaeggi_short-_2011}; \cite{muraven_self-regulation_2000}). Thus, we predicted that we could affect MB decision processes by training participants on tasks involving forward-planning and mental simulation, whereas training on tasks involving habitual, reward-driven behavior should affect the MF process. Here, we test this prediction in a behavioral study. The study was pre-registered on the Open Science Framework prior to data collection (osf.io/nw9vz).

\section{Methods and Results}
\subsection{Study Design and Description of the Tasks}

116 participants took part in the two-session experiment. In session 1, all participants first performed the 2-step decision making task (\cite{daw_model-based_2011}, see description below; Figure \ref{TwoStep}B), then received one of three training interventions designed at testing our hypothesis, then were tested on the 2-step task again (run 2). Participants came back for a third assessment of the 2-step task 2 days later (run 3; Figure \ref{TwoStep}A).% first high level description of the protocol.

Participants were randomly assigned to one of three interventions: model-based (MB), model-free (MF), or control. We chose tasks that were well established in the literature for engaging cognitive and neural processes corresponding to each mode of decision making (MB and MF), or for not engaging MB or MF processes (control). For training, MB participants engaged in two tasks that were closely related to MB decision making: a planning-intensive version of the Tower of London task (\cite{beauchamp_dynamic_2003}), and a rule-based category learning task (\cite{maddox_dissociating_2004}). Both tasks engage model-based planning or cognitive control, and rely on the brain network required for MB decision making (\cite{dolan_goals_2013}). MF participants engaged in tasks targeted at MF processes: a habitual reward-based task (\cite{tricomi_value_2015}) and an information-integration category learning task (\cite{maddox_dissociating_2004}). Both tasks engage long-term information integration and habitual behaviors, and rely on brain regions underlying MF decisions (\cite{dolan_goals_2013}). Finally, the control tasks (number comparison task: \cite{piazza_neurocognitive_2010}; orientation discrimination task: \cite{sasaki_advances_2010}) were unrelated to MB or MF decision making. 
%
\begin{figure}
	\includegraphics[width=\linewidth]{TwoStepab2.png}
	\caption{Experimental procedure (A) and 2-step task (B). (A) as described in the main text. (B) 2-step task: each trial has two successive states, $s_{1}$ and $s_{2}$. In $s_{1}$, participants decide between two actions $a_{1}$ (blue vs pink). One choice (e.g. blue) is followed by state $s_{2a}$ most of the time ("common transition"), the other one (pink) by state $s_{2b}$. After deciding between two actions in state $s_{2}$, participants receive a reward with probability $P(R|a_{2})$. Reward probabilities change over time to ensure continued learning.}
	\label{TwoStep}
\end{figure}
%
Participants' decision strategies were assessed using the 2-step task (\cite{daw_model-based_2011}; see Figure \ref{TwoStep}B). On each trial, this task involves two sequential binary decisions, which potentially lead to reward. The first decision determines with high probability what the second set of choices will be; however, in rare transition cases, the opposite set of choices is offered instead. This task is designed so that MB and MF strategies are distinguishable. A MF agent learns the values of a state-one action $a_{1}$ by accumulating the rewards obtained in trials in which $a_{1}$ was selected; future choices of $a_{1}$ are therefore mainly driven by past reward, independently of whether the trial included a common or rare transition between $s_{1}$ and $s_{2}$. A MB agent, on the other hand, selects $a_{1}$ based on its knowledge of the task structure, taking into account the transition probabilities between state $s_{1}$ and $s_{2}$ when reasoning which first action $a_{1}$ to take. Future actions $a_{1}$ therefore depend on both past rewards and transition probabilities. Specifically, MB agents tend to repeat the same $a_{1}$ upon reward when the transition was common, but they tend to select the alternative $a_{1}$ upon reward when the transition was rare. The opposite pattern emerges in unrewarded trials.

\subsection{Behavioral Switch-Stay Analysis}
We tested 116 participants in this paradigm. Standard procedures were used for data cleaning and exclusion of participants, resulting in 303 datasets from 114 participants (110 in run 1: 44 MB, 43 MF, 21 control; 103 in run 2: 43 MB, 41 MF, 19 control; and 90 in run 3: 36 MB, 35 MF, 19 control). We first analyzed the 2-step task using logistic regression to assess the effect of rewards and transitions on future choices (\cite{akam_simple_2015}; see Figure \ref{Results}A and B). Participants tended to repeat rewarded actions more often than unrewarded actions, a sign of MF behavior. This effect was statistically significant in all runs in the MF and MB groups, but not in the control group, as revealed by main effects of previous reward on staying, in logistic mixed-effects regression models including the factors choice repetition and key repetition (control group: all $\beta's < 0.12$, $z's < 1.61$, $p's > 0.11$; MB: all $\beta's > 0.15$, $z's > 3.79$, $p's < .001$; MF: all $\beta's > 0.18$, $z's > 3.41$, $p's < .001$). Besides this MF component, participants also showed markers of MB decision making, revealed by a significant positive interaction between reward and transition in run 2 (control: $\beta = 0.11$, $z = 2.05$, $p = .040$; MB: $\beta = 0.11$, $z = 2.54$, $p = .011$; MF: $\beta = 0.10$, $z = 2.92$, $p = .0035$). We then tested for differences between groups, using interaction contrasts. We found no differences in the MF or MB component for any run, shown by non-significant interactions between reward and group, all $\chi^{2}(2) < 1.33$, $p's > .40$, and between reward, transition, and group, all $\chi^{2}(2) < 1.17$, $p's > .56$. Nevertheless, the model-based component changed over time, as revealed by the interaction between reward, transition, and run ($\chi^{2}(4) < 15.07$, $p = .0046$). This change over time did not differ by training, revealed by the non-significant interaction of this effect with training ($\chi^{2}(16) < 6.04$, $p = .99$). % Not sure if this last results is important - what do you think?

In summary, the MB and MF groups showed MF characteristics in all runs and additional MB components in run 2. The control group showed no sign of MF decision making, but MB decisions in run 2. These results were confirmed by regression models integrating a larger number of previous trials to predict actions (\cite{akam_simple_2015}; results not shown).

\subsection{RL Modeling Analysis}
The previous analyses model choices based on the characteristics of a single previous trial. We followed these analyses up with computational modeling to assess participants' decision strategies as a mixture of long-term MB vs. MF reinforcement learning (\cite{akam_simple_2015}; \cite{daw_model-based_2011}; \cite{sutton_reinforcement_2017}). We specified a hybrid model, in which agents determine action values by combining MB and MF value estimates. We then fit this model to each participant's actions by selecting parameter values that maximized the likelihood of the observed actions under the model. The parameter $w$, which determines the weight of MB and MF value estimates, was used to assess participants' decision strategy.

Our model was similar to previously published models of this task (e.g., \cite{wunderlich_dopamine_2012}). Specifically, agents updated action values $Q$ for actions $a_{2}$ available in the second state $s_{2}$ by observing the trial outcome (reward $R = 1$ or $R = 0$), as follows:
%
\begin{equation}
Q(s_{2}, a_{2}) = Q(s_{2}, a_{2}) + \alpha_{2} \cdot RPE,
\end{equation}
%
where the reward prediction error $RPE = R - Q(s_{2}, a_{2})$ and $\alpha_{2}$ is the agent's learning rate in the second state. The update of first-stage action values $Q(s_{1}, a_{1})$ differed between MB and MF agents. MF agents used the outcome of $a_{1}$ to update $Q(s_{1}, a_{1})$. The outcome of $a_{1}$ consists in the value of the action chosen in $s_{2}$ and the trial's reward $R$.
%
\begin{equation}
Q_{mf}(s_{1}, a_{1}) = Q_{mf}(s_{1}, a_{1}) + \alpha_{1} \cdot (VPE + \lambda \cdot RPE),
\end{equation}
%
where the value prediction error $VPE = Q_{mf}(s_{2}, a_{2}) - Q_{mf}(s_{1}, a_{1})$. The weight of the RPE was determined by $\lambda$, a temporal discounting factor.

MB agents determined $Q(s_{1}, a_{1})$ based on an internal predictive model, taking into account the transition probability $p(s_{2}, a_{1}, s_{1})$ between state $s_{1}$ and state $s_{2}$, when choosing action $a_{1}$, and planning to select the best available action $a_{2}$:
%
\begin{equation}
Q_{mb}(s_{1}, a_{1}) = \sum_{s_{2}} p(s_{2}, a_{1}, s_{1}) \cdot max(Q(s_{2}, a_{2})).
\end{equation}
%
Agents combined MB and MF value estimates using a weighted average, $Q_{hyb}(s_{1}, a_{1}) = (1 - w) \cdot Q_{mf}(s_{1}, a_{1}) + w \cdot Q_{mb}(s_{1}, a_{1})$, whereby the parameter $w$ determined the weight of MB versus MF values. Agents selected actions $a$ according to a softmax decision rule, which took into account the action's value $Q(s, a)$, but also whether the same action had been taken in the previous trial (choice perseverance $p$) and whether the same key had used to select it (key perseverance $k$). The inclusion of $k$ is an extension of previous models and improved the model fit significantly. We validated our model by simulating agents with different parameter values and subsequently recovering these parameters (data not shown).
%
\begin{figure}
	\includegraphics[width=\linewidth]{Results.png}
	\caption{Results of the 2-step analyses. (A) Participants' probability of repeating the action $a_{1}$ taken in the previous trial ("s1 stay probability"), as a function of reward and transition in the previous trial. Error bars indicate standard errors. (B) Beta weights of logistic regression predicting staying (repeating previous $a_{1}$) from previous reward, transition, and their interaction. Empty circles: non-significant; filled circles: significant at $p < 0.05$. (C) Means and standard errors of parameter $w$ fit to individual datasets.}
	\label{Results}
\end{figure}
%
We then aimed to characterize human performance in terms of the model parameters, specifically parameter $w$ indicating the balance between MB and MF decision making. Model comparison with Bayesian Information Criterion (BIC) indicated that a full hybrid model with fixed future discounting $\lambda=1$ was best (Wilcoxon signed-ranks test, $w = 41186$, $p = .029$). In accordance with the previous analyses, model fitting results (Figure \ref{Results}B) showed that $w$ increased in run 2 in the control and MF groups. Nevertheless, this effect was not statistically significant in a mixed-effects regression model testing for effects of group and run on $w$ (group: $\chi^{2}(2)=0.16$, $p = .92$; run: $\chi^{2}(2)=1.41$, $p = .50$).

\section{Conclusion}
In this study, we aimed at investigating factors that influence human decision making. Specifically, we tested whether the use of MB versus MF strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would influence MB decision strategies, whereas habitual, reward-driven stimulus-response behavior would influence MF decision making. Contrary to our hypothesis, we found no significant changes in decision strategy over training, and no differences between intervention groups and control with regard to MB and MF components.

One reason for this negative result might be that the training tasks were conceptually too different from the later assessment. Indeed, so-called "far" transfer effects have been observed less commonly than training effects within similar tasks (\cite{jaeggi_short-_2011}). Another reason might be that the carry-over of cognitive processing between the training tasks and the 2-step assessment receded before the end of the 2-step task. Indeed, the 2-step task takes up to 20 minutes, and carry-over effects might not have persisted throughout. If this was the case, a shorter assessment of decision strategy might lead to better results. A last difficulty was that the control group showed different initial decision making than the two intervention groups, despite big sample sizes in each group (22, 46, and 48 participants). This makes it more difficult to observe differences in changes over time. Nevertheless, even the two intervention groups, with similar initial decision strategies, did not differ from one another. This supports the notion that increasing MB decision making experimentally is very difficult. Indeed, no such effect has been reported in the literature, despite various attempts.

Surprisingly, we found that participants in the MB and MF groups developed an unexpected decision pattern during training: in run 3, participants were more likely to repeat actions that were followed by common rather than rare transitions, as revealed by a significant interaction contrast between transition and group (control vs active), $\beta = 0.041$, $z = 2.23$, $p = .026$. This effect is not expected from either MB or MF decision making. Instead, it might reflect a preference for choices that lead to predicted outcomes. However it is unclear why this preference arose in the active groups, but not in the control group. Such a pattern has not been observed in the previous literature and might be interesting to pursue in future research.

Looking forward, research into the manipulation of MB and MF decision strategies is of great relevance. MF decisions are fast and efficient, but can be suboptimal in the long term (eating the chocolate cake). MB decisions (preparing for an exam) require more effort but often lead to better long-term outcomes when knowledge about environmental contingencies is relevant. Many people, including persons with mental health problems such as ADHD, depression, or eating disorders, would benefit from a cognitive intervention that facilitates MB decision making and trains selecting appropriate decision strategies. Further research will be needed to establish how MB behavior can be encouraged in decision making.

\section{References}

\printbibliography

\end{document}
