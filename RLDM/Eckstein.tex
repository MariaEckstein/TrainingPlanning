\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology \\
University of California, Berkeley \\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Department of Psychology \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
Department of Psychology\\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
The \emph{title} should be a maximum of 100 characters. 

The \emph{abstract} should be a maximum of 2000 characters of text,
including spaces (no figure is allowed). You will be asked to copy
this into a text-only box; and it will appear as such in the
conference booklet. Use 11~point type, with a vertical spacing of
12~points.  The word \textbf{Abstract} must be centered, bold, and in
point size 12. Two line spaces precede the abstract.
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{The German Academic Exchange Service (DAAD) supported this research with a fellowship for graduate studies to the first author. The current study was made possible by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment to the first author.}  


\startmain % to start the main 1-4 pages of the submission.

\section{Questions:}
\begin{itemize}
	\item Do the same preprocessing for behavioral and model analysis? (Exclude fast trials (100msec); choice-1 variation; key perseverance; exclude first 20 trials)
	\item Do I exclude key rep trials if I include key rep in the regression models?
	\item Plot absolute switch-stay probs, or relative to each subject's mean?
\end{itemize}

\section{TDs:}
\begin{itemize}
	\item Find model with the best BIC
	\item Check which model fits best for each session
	\item Relation between task performance and mb/mf outcome?
	\item Decide on \& make figures
	\item (Learn how to) insert figures
	\item Do some more research on mb / mf studies?
\end{itemize}

At the very end:
\begin{itemize}
	\item Make equations pretty
	\item Use BIBTeX to create citations
\end{itemize}

\section{Introduction}

Humans make many decisions and solve a large number of problems every day: Which restaurant to pick for dinner, which books to select for reading, what to do on the weekend. What strategies do humans use to make such decisions? In the behaviorist tradition of psychology (Skinner, Watson, Pavlov), researchers have contended that all observed behavior, ranging from button pressing in rats to language production in humans, can be explained in terms of responses elicited by a specific stimulus (Skinner, 1977). In this view, all behavior is the product of shaping, in which the reward characteristics of the environment lead to the formation of stable stimulus-response associations. According to this view, we pick a restaurant for dinner, with which we have had good experiences so far. Cognitive psychologists, on the other hand, have contended that humans and animals possess internal representations of their environment and are able to reason about causal relationships and contingencies (Tolman, 1948). In the cognitivist view, agents choose specific actions based on mental simulations of their likely consequences. In this view, we'd pick the restaurant that we reason fulfills a list of relevant criteria.

These two opposing characterizations of decision making, behaviorist and cognitivist, are mirrored in the third big framework of machine learning beside supervised and unsupervised learning---in reinforcement learning (Sutton \& Barto, 2012). In reinforcement learning, sequential tasks are characterized as Markov Decisions Problems (MDPs), in which certain states are associated with rewards. Actions allow agents to transfer between states. An agent's goal is to maximize rewards in the long run, i.e., to learn an optimal mapping between states and actions, so as to choose the path that leads to the most rewards. Such an optimal policy is often achieved by learning values, which indicate for each action in each state, how much reward is expected when taking this action in this state. Model-free agents determine values by averaging over the past reward history following each state-action pair (Monte Carlo), similar to the behaviorist framework in psychology. Model-based agents, on the other hand, use an internal model of the environment to simulate action outcomes, similar to the cognitivist tradition.

Bringing psychology and machine learning together, past research has shown that human and animal behavior can be modeled using reinforcement learning algorithms. A big success was the finding that activity in the brain's dopamine system, sometimes called the brain's "reward system" (Wise, 1982), coincided precisely with the occurrence of reward prediction errors, central components of reinforcement learning (Schultz, 1997). Much interest has also been in the differentiation of model-based and model-free reinforcement learning in animals and humans. Daw and colleagues (year) developed a behavioral task that aims at differentiating model-based and model-free components in humans, the "2-step task". Individuals differ in their propensities toward model-based or model-free decision making, but this balance is also situation-dependent. The balance tips toward model-free decision making, fast and with lower cognitive demands, during stress (Schwabe \& Wolf, 2011) or when multiple tasks are performed simultaneously (Otto, Gershman, Markman, \& Daw, 2013). However, it has not been shown how the balance could be tipped the other way, to increase model-based decisions. The only study that has achieved this---to the best of our knowledge---was a pharmacological manipulation of dopamine levels, rather than a cognitive manipulation (Wunderlich, Smittenaar, \& Dolan, 2012).

Where?? model-based: frontal cortex and dorsal striatum; model-free: subcortical regions and ventral striatum; Dolan \& Dayan, 2013

The current study aims to investigate whether cognitive interventions can manipulate the balance between model-based and model-free decision making, going both ways. We hypothesized that decision making strategies would depend on the kind of cognitive processes that participants engaged in before. Engaging in forward-planning and mental simulation before assessment should foster model-based decisions, whereas engaging in habitual, stimulus-driven behavior should foster model-free decisions. Second, we hypothesized that the same would hold for brain areas known to underlie model-based (frontal cortex and dorsal striatum) versus model-free decision making (subcortical regions and ventral striatum; Dolan \& Dayan, 2013). Prior tasks that activate one set should foster model-based decisions, whereas the other set should increase model-free decisions. Lastly, tasks that are unrelated to model-based and model-free processes and brain areas should have no influence on subsequent decision making and allow for a baseline assessment of model-based and model-free decisions.

Based on these hypotheses, we pre-registered the study prior to data collection on the Open Science Framework (www.osf.org/...).

\section{Methods and Results}
\subsection{Study Design}

We designed the following cognitive intervention based on our hypotheses: We asked 116 participants to take part in a two-session experiment, with the session taking place two days apart. We assessed participants' decision strategies using Daw et al.'s 2-step task at the beginning and end of each session, i.e., four times in total. Participants were randomly assigned to one of five groups. The control group engaged in the same two control tasks during the first and second session, a number comparison task (cite), and an orientation-discrimination task (cite). The model-based group (MB-MB) received the Tower of London task (cite) and a rule-based category task (Ashby \& Maddox) during both sessions and the model-free group (MF-MF) received an operant condition task (cite) and an information-integration category (cite) task in both sessions. The last two groups of participants were mixed: the MB-MF group received the tasks of the MB-MB group in the first session, and of the MF-MF group in the second session. The opposite was true for the MF-MB group. The analyses presented here focus on the first three 2-step assessments and collapse over the MB-MB and MB-MF groups, who had only received the model-based intervention up to that point. Likewise, we collapses over the MF-MF and MF-MB groups who had only engaged the model-free tasks yet.

In each trial of the 2-step task, with which we assessed model-based and model-free behavior, participants select one of two stimuli in the first stage (e.g., blue vs green). Depending on the selected stimulus (e.g., green), participants are then presented with a second binary decision (e.g., red vs yellow stimulus). The second choice determines whether participants are rewarded or not. Each stimulus in the second stage is associated with a certain, independent reward probability (which slowly changes over time). Crucially, the link between first-stage choice and second-stage pair is probabilistic, rather than deterministic. In our example, selecting the green stimulus in the first stage would lead to the read-vs-yellow stage in 70\% of cases, but to a different, e.g., pink-vs-purple stage in 30\% of the trials. The opposite would be true for the blue stimulus in the first stage.

In this task, a model-free agent learns the values of each first-stage stimulus by averaging the rewards obtained after selecting this stimulus. It does not discriminate between common and rare transitions between the first and second stage. A model-based agent, on the other hand, selects actions based on its knowledge about the structure of the task, including common and rare transitions, and therefore differentiates between common and rare transitions.

\subsection{Behavioral Switch-Stay Analysis}
Participants' decision making strategies in the 2-step task can be analyzed in two ways: Classical behavioral analyses assess whether participants differentiate between common and rare transitions. RL modeling tests for model-based and model-free learning explicitly by fitting the corresponding models to the human data.

Before the analyses, we cleaned the behavioral data, using standard procedures. We excluded trials, which were part of a streak in which participants pressed the same key more than 15 times in a row (the presentation side of the stimuli switched randomly). An average of 17.7 out of 180 trials was excluded per subject. We then excluded trials in which participants responded faster than 100 msec. An average 11.2 trials per subject was excluded. Whole blocks of data were excluded when participants selected the same first-stage stimulus in more than 95\% of trials, which would otherwise lead to problems with the statistical techniques (17 blocks were excluded). Finally, blocks were excluded in which more than 75\% of the data had been removed during cleaning (81 blocks). The resulting data contained 332 blocks from 109 participants.

We first used the behavioral approach to analyze the data. We assessed participants' tendencies to repeat first-stage decisions after receiving rewards. As explained above, a model-free agent would repeat a rewarded action irrespective of whether it was followed by a common or rare transition. A model-based agent, on the other hand, would consider picking to other first-stage action after a rewarded rare transition because in this case, the probability of achieving the same outcome is higher when the opposite first-stage action is chosen. Figure \ref{Fig1} A shows that all three groups of participants tended to repeat rewarded actions more often than unrewarded actions, a sign of model-free behavior. This effect was statistically significant, as revealed by the main effect of reward on staying in most groups and runs (Figure \ref{Fig1} B), revealed by logistic, mixed-effects regression using the lme4 package (cite) in R (cite). Besides this model-free component, participants also showed model-based decision making in that they differentiated between common and rare transitions. This was reflected in the interaction between reward and transition in some of the regression models (Figure \ref{Fig1} B).

\begin{figure}
	\includegraphics[width=\linewidth]{Fig1ab.png}
	\caption{bla.}
	\label{Fig1}
\end{figure}

We next tested for differences in decision making between groups, but the regression models revealed no significant differences. There were no interactions at any time point between reward and group, which would have indicated differences in the model-free component (stats). Nor were there interactions between reward, transition, and group, which would have indicated differences in the model-based component (Run 1: group*reward, p = 0.55, ...). Lagged regression models (cite) and logistic regression model on the individual data supported this conclusion.

\subsection{RL Modeling Analysis}
We next employed classical RL models to assess participants' decision process in more detail. In the models, agents chose actions $a$ based on internally-represented action values $Q$. Action values indicate how much reward an agent expects from choosing an action. In order to learn action values, agents observe the outcome of each trial (reward $R = 1$ or $R = 0$) and update the value $Q(s', a')$ of the selected second-stage action $a'$ in the second stage $s'$, in the following way:

\begin{equation}
Q(s', a') = Q(s', a') + \alpha' * RPE,
\end{equation}
\begin{equation}
RPE = R - Q(s', a')
\end{equation}

where $ \alpha' $ is an agent's learning rate, specific to learning of second-stage action values. Having updated the value $Q(s', a')$ of the selected second-stage action, agents then update the values $Q(s, a)$ of first-stage actions. Model-free agents do the following:

\begin{equation}
Q_{mf}(s, a) = Q_{mf}(s, a) + \alpha * (VPE + \lambda * RPE)
\end{equation}
\begin{equation}
VPE = Q(s', a') - Q_{mf}(s, a)
\end{equation}

The value of the first-stage action depends on the reward prediction error (RPE) and a value prediction error (VPE), the difference between the value (i.e., expected reward) of the first action and the value of the received second state. The weight of the RPE for first-stage updating was determined by $ \lambda $, a temporal discounting factor. A model with $ \lambda = 0 $ was the best-fitting model for our participants, so only differences in values were taken into account in first-stage action updating, not rewards.

Model-based agents determined the values of first-stage actions based on their internal predictive model, taking into account their known transition probabilities $p(s', a, s)$ between state $s$ and state $s'$, when choosing action $a$:

\begin{equation}
Q_{mb}(s, a) = \sum_{s'} p(s', a, s) * max(Q(s', a'))
\end{equation}

Model-based agents therefore updated first-stage values by calculating the expected rewards from each action, given the transition probabilities between first and second states.

We modeled participants as hybrid agents who combined model-based and model-free value estimates for first-stage actions:

\begin{equation}
Q_{hyb}(s, a) = (1 - w) * Q_{mf}(s, a) + w * Q_{mb}(s, a)
\end{equation}

The parameter $w$ determines the weight of model-based and model-free values to determine the value of the first-stage action, which the agent used to select the action.

Agents selected actions $a$ and $a'$ in the following way, based on their action values $Q(s, a)$ and $Q(s', a')$:

\begin{equation}
P(a|s) = \frac{exp(\beta * Q(s, a) + k * K + p * P)}{\sum_{i} exp(\beta * Q(s, a_{i}) + k * K + p * P)}
\end{equation}

$\beta$ is the agent's individual softmax temperature. $\beta$ determines whether the agent will always choose the action with the highest value, or explore both actions more equally. Different values of $\beta$ were allowed for the first and second state ($\beta$ and $\beta'$). Besides action values, participants action selection was also determined by whether the same action was chosen in the previous trial ($P$) and whether the same key was pressed in the previous trial ($K$). The weight of these factors were determined by weights $p$ and $k$, which varied by agent. The model fit only improved with the inclusion of choice perseverance $P$, but not key perseverance $K$, 
Explain keyrep and fracrep.

We validated our model by simulating agents with different parameters, and subsequently recovering these parameters (data not shown). We also confirmed that reduced models, with certain parameters fixed, were best recovered by models with the same free parameters, rather than different models. 

Having confirmed the validity of our model, we next aimed to characterize human performance in terms of the free parameters of the model---learning rates alpha1 and alpha2, decision temperature beta1 and beta2, future discounting lambda, key perseverance k, choice perseverance p, and balance between model-based and model-free decision making w. To this aim, we first determined if a simplified model fit the behavioral data better than the full model. Indeed, the Bayesian Information Criterion (BIC) was lower for a reduced model with only 5 parameters---keeping key perseverance at k = 0 and discounting lambda at 0.

We then fit the simplified model to individual participants' behavior. To this aim, we selected the free parameters such that the likelihood of the participant's responses was maximized. We used Matlab's fmincon function with 30 different start values to find the best-fitting parameter values for each participant, minimizing the negative log of the likelihood. Each participant's data were thereby associated with a specific set of parameters, characterizing this participant's behavior. The results are shown Figure 3. We found that, contrary to our expectations but in line with the behavioral analyses, different training tasks did not lead to differences in w, the balance between model-based and model-free decisions. 

On an individual subject level, we found changes between assessments, but nothing consistent for the whole group.

\section{Conclusion}
We present the results of a study aimed at investigating factors that influence human decision making. In specific, we aimed to test whether the use of model-based versus model-free strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would facilitate model-based decision strategies, whereas habitual, reward-driven, stimulus-response behavior would promote model-free decision making. We selected tasks with these cognitive components that had also been (excessively) studied using functional magnetic resonance imaging (fMRI), so that we can conclude that the model-based set of tasks (ToL, rb category) engaged brain regions that have been associated with model-based decision making, whereas the model-free tasks engaged brain regions associated with model-free decision making. 

Our study revealed no influence of the cognitive intervention tasks on decision making. This can not be attributed to limited power because the samples size (25 participants per group) had been determined based on a power analysis in a pilot sample, which had shown promising results. 

Instead, the results suggest that previous cognitive activities do not influence subsequent decision making---at least at the scale of the duration of the 2-step task (approximately 15 minutes???). It is possible that carry-over effects from previous tasks exist on a shorter time scale, but we are not aware of a shorter test of decision making strategy than the used 2-step task. Lastly, the discrepancy between the pilot data, in which there were hints of an effect, and the current study might also arise from the fact that participants did the 2-step task 4 versus 2 times. Maybe it only works when you have never seen it before.

Concluding, research into the manipulation of model-based and model-free decision strategies is of great relevance. Model-free decisions are often short-sighted and suboptimal on the long term (the chocolate cake). Model-based decisions require more effort but can lead to better long-term outcomes. Many vulnerable subjects, including persons with ADHD, depression, schizophrenia, Parkinson's disease, obesity, impulsitiy, eating disorders, or addiction would benefit from a cognitive intervention that facilitates model-based decision making and creates some ease in selecting and switching between both strategies, according to the context.

\section{References}


\end{document}
