\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage[style=apa,backend=biber]{biblatex}
\addbibresource{TwoStep.bib}
\DeclareLanguageMapping{english}{american-apa}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology \\
University of California, Berkeley \\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Department of Psychology \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
Department of Psychology\\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Abstract. 
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{This research was supported by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment as well as by a fellowship from the German Academic Exchange Service (DAAD).}  


\section*{TDs:}
\begin{itemize}
	\item Words: "rare" and "common" or "frequent" or ...?
	\item Words: "model-based" or "MB" and "model-free" or "MF"?
	\item Do some more research on mb / mf studies
	\item Abstract
	\item Simulate data using participants' parameter values -> recover
\end{itemize}

\startmain

\section{Introduction}

Humans make many decisions habitually: for example, we effortlessly navigate the route we take every day, following a fixed sequences of actions and using landmarks to trigger subsequent actions. But humans also make decisions in a goal-directed way. For example, when we plan how to reach a goal, we flexibly combine individual pieces into a new route, using a cognitive map or model of our environment. These two different modes of decision making, habitual and goal-directed, have long been differentiated in psychology, and form the basis of two different schools of thoughts, namely Behaviorism (\cite{skinner_why_1977}) and cognitivism (\cite{tolman_cognitive_1948}). A parallel differentiation exists in computational models of reinforcement learning, a branch of machine learning. Here, the distinction is between model-based (MB; similar to goal-directed) and model-free (MF; similar to habitual) agents. MB agents use a model of the environment to simulate possible actions and outcomes, and then determine which actions are expected to lead to the best outcomes. In contrast, MF agents determine the value of actions by accumulating the past reward history of these actions.

Recently, reinforcement learning algorithms have been applied to psychological research. This has led to the striking discovery that activity in the dopamine system, the brain's "reward system" (\cite{wise_brain_1989}), coincides with the occurrence of reward prediction errors as specified in MF reinforcement learning (\cite{schultz_neural_1997}). MB learning has been shown to rely on a distinct brain network (including frontal cortex and dorsal striatum; \cite{dolan_goals_2013}). Human learning and decision making relies on both MB and MF processes (\cite{daw_model-based_2011}), and a key question is how we arbitrate between the two processes. Previous studies have shown that cognitively demanding MB decision making is less prevalent when time and/or cognitive resources are sparse, for example during stress (\cite{schwabe_stress-induced_2011}) or when multi-tasking (\cite{otto_curse_2013}). On the other hand, no situations have yet been identified that increase MB decision making. The only study that has shown an increase in MB decision making was a pharmacological manipulation of dopamine levels (\cite{wunderlich_dopamine_2012}). 

In the current study, we therefore sought to investigate whether a cognitive intervention could increase MB decision making. We hypothesized that decision strategies are influenced by prior cognitive activities. Forward-planning and mental simulation should influence the MB process, whereas habitual, reward-driven behavior should influence the MF process. This influence could go either way, fostering (cognitive enhancement hypothesis; \cite{jaeggi_short-_2011}) or depressing (cognitive depletion hypothesis; \cite{muraven_self-regulation_2000}) the subsequent decision strategy. An relative increase of MB vs. MF decision making after MB activities would support the cognitive enhancement hypothesis; the same increase after MF activities would support the cognitive depletion hypothesis. We pre-registered these hypotheses on the Open Science Framework prior to data collection (www.osf.io/).

\section{Methods and Results}
\subsection{Study Design and Description of the Tasks}

116 participants took part in a two-session experiment. All participants performed the 2-step decision making task (\cite{daw_model-based_2011}, see description below; Figure \ref{TwoStep}B), then received one of three training interventions designed at testing our hypothesis, then were tested on the 2-step task again; participants came back for a third assessment of the 2-step task 2 days later (Figure \ref{TwoStep}A).% first high level description of the protocol.

Participants were randomly assigned to one of three interventions: model-based intervention (MB), model-free intervention (MF), and control. We chose tasks that were well established in the literature for engaging cognitive and neural processes corresponding to each mode of decision making (MB and MF), or for not engaging MB or MF processes (Control). For training, MB participants engaged in two tasks that were closely related to MB decision making: a planning-intensive version of the Tower of London task (\cite{beauchamp_dynamic_2003}), and a rule-based category learning task (\cite{maddox_dissociating_2004}). Both tasks engage model-based planning or cognitive control, and rely on the brain network required for MB decision making (\cite{dolan_goals_2013}).%The Tower of London task is model-based because it requires participants to use an internal model of the task to simulate potential moves and their outcomes, and to select the best sequence of moves. The rule-based category task has been shown to rely on model-based mechanisms, for example in that learning is unaffected by delayed feedback (Ashby and Maddox). Indeed, both Tower of London and the rule-based category learning task engage brain regions similar to model-based decision making (Beauchamp, Dagher, Aston \& Doyon, 2003; Ashby and Maddox).
% you can keep the details if there's room, but it's probably not needed if you're lacking space.
MF participants engaged in tasks targeted at MF processes: a habitual reward-based task (\cite{tricomi_value_2015}) and an information-integration category learning task (\cite{maddox_dissociating_2004}). Both tasks engage long-term information integration and habitual behaviors, and rely on brain regions underlying MF decisions (\cite{dolan_goals_2013}). %The habitual reward-based task is a classical example of model-free, habitual learning, where participants were rewarded when they pressed the correct button in response to simple visual stimuli. The information-integration category task relies on model-free processes, as is evident in that performance is negatively affected by interventions that disrupt model-free learning, such as lack of immediate feedback. Again, both tasks have been shown to engage brain regions underlying model-free decisions. 
Finally, the control tasks were unrelated to MB or MF decision making: The number comparison task involves the approximate number sense and the orientation discrimination task involves perceptual learning, associated with brain regions other than those involved in MB or MF decision making (\cite{piazza_neurocognitive_2010}; \cite{sasaki_advances_2010}). 
%The number comparison task engages the approximate number sense and X and Y brain regions (Piazza, 2010). The orientation discrimination task involves perceptual learning and early visual processing and is associated with activity in the retinotopic region of V1 corresponding to the location of stimulus presentation (Kim, Ling, Watanabe, 2015). 

\begin{figure}
	\includegraphics[width=\linewidth]{TwoStepab2.png}
	\caption{Experimental procedure (A) and 2-step task (B). (A) as described in the main text. (B) 2-step task: each trial has two successive states, $s_{1}$ and $s_{2}$. In $s_{1}$, participants decide between two actions $a_{1}$ (blue vs pink). One choice (e.g. blue) is followed by state $s_{2a}$ most of the time ("common transition"), the other one (pink) by state $s_{2b}$. In state $s_{2}$ participants decide between two actions and receive a reward with probability $P(R|a_{2})$ determined by $a_{2}$. Reward probabilities change over time to ensure continued learning.}
	\label{TwoStep}
\end{figure}

Participants' decision strategies were assessed using Daw et al.'s (\cite{daw_model-based_2011}) 2-step task (see Figure \ref{TwoStep}B). On each trial, this task involves two sequential binary decisions, which potentially lead to reward. The first decision determines with high probability what the second set of choices will be; however, in rare transition cases, the opposite set of choices is offered instead. This task is designed so that MB and MF strategies are distinguishable. A MF agent learns the values of a state-one action $a_{1}$ by accumulating the rewards obtained in trials in which $a_{1}$ was selected; future choices of $a_{1}$ are therefore mainly driven by past reward, independently of whether the trial included a frequent or rare transition between $s_{1}$ and $s_{2}$. A MB agent, on the other hand, selects $a_{1}$ based on its knowledge of the task structure, taking into account the transition probabilities between first decision state $s_{1}$ and second decision state $s_{2}$ when reasoning which first action $a_{1}$ to take. Future actions therefore depend on both past rewards and transition probability. Specifically, MB agents tend to repeat $a_{1}$ upon reward when the transition common, but they tend to select the alternative $a_{1}$ upon reward when the transition was rare. The opposite behavior emerges in unrewarded trials.

\subsection{Behavioral Switch-Stay Analysis}
%The 2-step task can be analyzed in two ways. Behavioral analyses assess the influence of past rewards on participants' subsequent decisions (model-free characteristic) and they asses whether participants differentiate between common and rare transitions (model-based characteristic). RL modeling tests for model-based and model-free decisions explicitly by fitting RL models to the human data and estimating the weight of model-based and model-free decision making.

We tested 116 participants, and used standard procedures to exclude participants (CITE). The resulting data consisted of 303 datasets from 114 participants (110 in run 1---44 MB, 43 MF, 21 control; 103 in run 2---43 MB, 41 MF, 19 control; and 90 in run 3---36 MB, 35 MF, 19 control). We first analyzed the 2-step task results using logistic regression to assess the effect of rewards and transitions on future choices (\cite{akam_simple_2015}; see Figure \ref{Results}A). Participants tended to repeat rewarded actions more often than unrewarded actions, a sign of MF behavior. This effect was statistically significant in all runs in the MF and MB groups, but not in the control group, as revealed by main effects of reward in the previous trial on staying in logistic mixed-effects regression models including the factors choice repetition and key repetition, INSERT STATS. Besides this MF component, participants also showed MB decision making in that they differentiated between common and rare transitions. This was reflected in the interaction between reward and transition in runs 2 and 3 in the control group, run 2 in the MB group, and in runs 2 and 3 in the MF group (although the pattern was reversed here in run 3, so should not be taken as support for a MB strategy), STATS. We then tested for differences between groups, using interaction contrasts. We found that the MF component did not differ between groups in any run, revealed by non-significant interactions between reward and group, STATS. The MB component, on the other hand, differed in run 3, as revealed by a significant interaction between reward, interaction, and group, STATS. This difference is caused by the more MB behavior in the control group than in MB and MF.

In summary, the control group showed predominantly MB behavior in runs 2 and 3. The MB and MF groups, on the other hand, showed MF characteristics in all runs and additional MB components in run 2. These results were confirmed by regression models integrating a larger number of previous trials to predict actions (not shown). These analyses also show that in run 3, MB participants differentiated between common and rare unrewarded trials, a sign of MB behavior. MF participants, on the other hand, treated common and rare trials similarly, a sign of MF behavior. The behavioral results were similar when only including data from participants who performed well on the training tasks.

\subsection{RL Modeling Analysis}
The previous analyses model choices based on the characteristics of a single previous trial. We followed these analyses up with computational modeling to assess participants' decision strategies as a mixture of long-term MB vs. MF reinforcement learning (\cite{sutton_reinforcement_2017}). We specified a hybrid model, in which agents determine action values by combining MB and MF value estimates. We then fit this model to each participant's actions by selecting parameter values that maximized the likelihood of the actions under the model. The parameter $w$, which determines the weight of MB and MF value estimates, was used to assess participants' decision strategy.

Our model was similar to the ones previously described for the 2-step task (e.g., \cite{wunderlich_dopamine_2012}). In specific, agents updated action values $Q$ for actions $a_{2}$ available in the second stage by observing the trial outcome (reward $R = 1$ or $R = 0$), as follows:

\begin{equation}
Q(s_{2}, a_{2}) = Q(s_{2}, a_{2}) + \alpha_{2} \cdot RPE,
\end{equation}

whereby the reward prediction error $RPE = R - Q(s_{2}, a_{2})$ and $\alpha_{2}$ is the agent's learning rate in the second state.

The update of first-stage action values $Q(s_{1}, a_{1})$ differed between MB and MF agents. MF agents used the outcome of $a_{1}$ to update $Q(s_{1}, a_{1})$. The outcome of $a_{1}$ consists in the value of the action chosen in $s_{2}$ and the trial's ultimate outcome ($R = 1$ or $R = 0$).

\begin{equation}
Q_{mf}(s_{1}, a_{1}) = Q_{mf}(s_{1}, a_{1}) + \alpha_{1} \cdot (VPE + \lambda \cdot RPE),
\end{equation}

whereby the value prediction error $VPE = Q_{mf}(s_{2}, a_{2}) - Q_{mf}(s_{1}, a_{1})$. The weight of the RPE for first-stage updating was determined by $ \lambda $, a temporal discounting factor. The model with $\lambda = 1$ was the best-fitting model for our participants, suggesting that participants weighted RPEs equally strong as VPE when updating action values $Q_{mf}(s_{1}, a_{1})$.

MB agents determined $Q(s_{1}, a_{1})$ based on an internal predictive model, taking into account the transition probability $p(s_{2}, a_{1}, s_{1})$ between state $s_{1}$ and state $s_{2}$, when choosing action $a_{1}$:

\begin{equation}
Q_{mb}(s_{1}, a_{1}) = \sum_{s_{2}} p(s_{2}, a_{1}, s_{1}) \cdot max(Q(s_{2}, a_{2}))
\end{equation}

Agents combined MB and MF value estimates using a weighted average, $Q_{hyb}(s_{1}, a_{1}) = (1 - w) \cdot Q_{mf}(s_{1}, a_{1}) + w \cdot Q_{mb}(s_{1}, a_{1})$, whereby the parameter $w$ determined the weight of MB versus MF values. Agents selected actions $a$ according to a softmax decision rule, which took into account the actions' values $Q(s, a)$, but also whether the same action had been taken in the previous trial (choice perseverance $p$) and whether the same key had been pressed in the previous trial (key perseverance $k$). The inclusion of key perseverance is an extension of previous models and improved the model fit significantly. %Individuals' softmax temperature with respect to action values $Q(s, a)$ was determined by parameters $\beta_{1}$ and $\beta_{2}$ for the fist and second state. 
We validated our model by simulating agents with different parameter values and subsequently recovering these parameters (data not shown). %We also confirmed in simulations that reduced models, with certain parameters fixed, were best recovered by models with the same free parameters, rather than different models. [AC deleted for space]

\begin{figure}
	\includegraphics[width=\linewidth]{Results.png}
	\caption{Results of the 2-step analyses. (A) Participants' probability of repeating the action $a_{1}$ taken in the previous trial ("s1 stay probability"), as a function of reward and transition in the previous trial. (B) Means and standard errors of model parameters fit to individual datasets. a1: $\alpha_{1}$, a1: $\alpha_{2}$, b1: $beta_{1}$, b2: $\beta_{2}$.}
	\label{Results}
\end{figure}

We then aimed to characterize human performance in terms of the model parameters, specifically parameter $w$ indicating the balance between MB and MF decision making. When fitting the model to human data, model comparison with Bayesian Information Criterion (BIC) indicated that a full hybrid model with fixed future discounting $\lambda=1$ was best (sign tests, STATS). %Matlab's fmincon function was used with 30 different start values to find the best-fitting parameters for each dataset. 
In accordance with the previous analyses, model fitting results (Figure \ref{Results}B) showed that $w$ increased in run 2 in the MF group. Nevertheless, this effect was not statistically significant in a mixed-effects regression model testing for effects of group and run on $w$.

\section{Conclusion}
In this study, we aimed at investigating factors that influence human decision making. In specific, we tested whether the use of MB versus MF strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would influence MB decision strategies, whereas habitual, reward-driven stimulus-response behavior would influence MF decision making. Contrary to our hypothesis, we found that MB decision making was highest in the control group, and potentially increased slightly in MB and MF in run 2, directly after the training. We found no long-term changes in decision strategy (run 3).

Considering the behavioral data (Figure \ref{Results}A), the biggest differences seemed to arise between the control group and the two active groups. The control group showed patterns expected in MB decision making, whereas MB and MF participants showed an unexpected pattern in run 3: Participants were more likely to repeat actions that were followed by a common rather than rare transition. This effect is not predicted by either MB or MF decision making. Instead, it might reflect a preference for choices that lead to predicted outcomes. However it is unclear why this preference arose in the active groups, but not in the control group. Such a pattern has not been observed in the previous literature and might be interesting to pursue in future research. %[AC Rephrase the last couple sentences, but this looks like a surprising effect, so you can try to spin that as a positive result in the study, and try to speculate why.]%But this does not match up with the modeling results...? [AC it can't be explained by those models]
% Regression models including trials further in the past show that this effect is restricted to just the previous trial in MB, but extends over many past trials in MF (data not shown). 

One reason for our negative result might be that previous cognitive activities only carry over very briefly into subsequent activities, such that the strategy in the whole 2-step task, which took approximately 20 minutes, was not affected throughout. If this is the case, a shorter assessment of decision strategy might find something. In addition, there were differences between the design of the pilot study, which had initially suggested short-term cognitive depletion and long-term cognitive enhancement. Namely, participants only did the 2-step task twice in the pilot study, at the end of the training session and in the 1-week follow-up. Plus, the 2-step training was interleaved with the intervention tasks, potentially increasing carry-over effects. In the current study, the 2-step task was the first task of all, potentially leading participants to determine their personal strategy and being less flexible in changing it.

Research into the manipulation of MB and MF decision strategies is of great relevance. MF decisions are fast and efficient, but may be suboptimal in the long term (eating the chocolate cake). MB decisions (preparing for an exam) require more effort but often lead to better long-term outcomes when knowledge about environmental contingencies is relevant. Many vulnerable subjects, including persons with ADHD, depression, schizophrenia, Parkinson's disease, obesity, impulsivity, eating disorders, or addiction would benefit from a cognitive intervention that facilitates MB decision making and creates some ease in selecting and switching between both strategies, according to the context. Further research will be needed to establish how MB behavior can be encouraged in decision making.

\section{References}

\printbibliography

\end{document}
