\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}

\title{Manipulating Model-based and Model-free Reinforcement Learning in Humans}


\author{
Maria K. Eckstein \\
Department of Psychology\\
University of California, Berkeley\\
Berkeley, CA 94720 \\
\texttt{maria.eckstein@berkeley.edu} \\
\And
Klaus Wunderlich \\
Ludwig Maximilian University, Munich \\
Geschwister-Scholl-Platz 1, 80539 Munich \\
\texttt{klaus.wunderlich@lmu.de} \\
\And
Anne Collins \\
University of California, Berkeley \\
Berkeley, CA 94720  \\
\texttt{annecollins@berkeley.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
The \emph{title} should be a maximum of 100 characters. 

The \emph{abstract} should be a maximum of 2000 characters of text,
including spaces (no figure is allowed). You will be asked to copy
this into a text-only box; and it will appear as such in the
conference booklet. Use 11~point type, with a vertical spacing of
12~points.  The word \textbf{Abstract} must be centered, bold, and in
point size 12. Two line spaces precede the abstract.
\end{abstract}

\keywords{
Reinforcement Learning Model-based Model-free Decision Making
}

\acknowledgements{The German Academic Exchange Service (DAAD) supported this research with a fellowship for graduate studies to the first author. The current study was made possible by Berkeley's XLab, which provided the testing space and awarded a grant covering participant payment to the first author.}  


\startmain % to start the main 1-4 pages of the submission.

\section{Introduction}

Humans make many decisions and solve a large number of problems every day: Which restaurant to pick for dinner, what books to read, how to convey information when asked a question. What strategies do humans use to make such decisions? In the behaviorist tradition of psychology (Skinner, Watson, Pavlov), researchers have contended that all observed behavior, ranging from button pressing in rats to language production in humans, can be explained in terms of responses elicited by a specific stimulus (Skinner, 1977). In this view, all behavior is the product of shaping, in which the reward characteristics of the environment lead to the formation of stable stimulus-response associations. Cognitive scientists, on the other hand, have contended that humans and animals possess internal representations of their environment and are able to reason about causal relationships and contingencies (Tolman, 1948). In the cognitivist view, agents choose specific actions based on mental simulations of their likely consequences.

These two opposing characterizations of learning and decision making, behaviorist and cognitivist theories, are mirrored in the third big framework of machine learning, beside supervised and unsupervised learning---in the framework of reinforcement learning (Sutton \& Barto, 2012). In reinforcement learning, sequential tasks are characterized as Markov Decisions Problems (MDPs), in which each state is associated with a reward (often 0). The actions available in one state allow the agent to transfer to a different state. The agent's goal is to maximize rewards in the long run, i.e., to create an optimal mapping between states and actions, so as to find the best path. The agent's mapping between states and actions is called its policy. Model-free approaches to policy search involve averaging over the past reward history of a given state-action pair (Monte Carlo), similar to the behaviorist framework in psychology. Model-based approaches, on the other hand, involve an internal model of the problem to simulate action outcomes, similar to the cognitivist tradition.

Bringing psychology and machine learning together, past research has shown that human and animal behavior can be modeled with reinforcement learning algorithms. Nathaniel Daw and colleagues (year) developed a behavioral task that aims at differentiating model-based and model-free components in humans, the "2-step task". It has been shown that individual participants differ in their propensities toward model-based or model-free decision making in this task (cite). This balance also depends on the situation. Model-free decision making, fast and cognitively less demanding, prevails in situations of stress (Schwabe \& Wolf, 2011) and when multi-tasking (Otto, Gershman, Markman, \& Daw, 2013), for example. Much less is known about situations that foster model-based decision strategies. The only study that increased the amount of model-based decision making---to the best of our knowledge---was a pharmacological manipulation of dopamine levels, a central neurotransmitter in the striatum (Wunderlich, Smittenaar, \& Dolan, 2012).

Specialized brain areas have been identified whose patterns of activity correspond to elements of the reinforcement learning algorithm, such as trial-by-trial reward prediction errors (Schultz, 1997) and characteristics of model-based versus model-free strategies (model-based: frontal cortex and dorsal striatum; model-free: subcortical regions and ventral striatum; Dolan \& Dayan, 2013).

The current study investigates whether cognitive interventions can manipulate the balance between model-based and model-free decision making, going both ways. We hypothesized that decision making strategies would depend on the kind of cognitive processes that participants engaged in before. Engaging in forward-planning and mental simulation before assessment should foster model-based decisions, whereas engaging in habitual, stimulus-driven behavior should foster model-free decisions. Second, we hypothesized that the same would hold for brain areas known to underlie model-based versus model-free decision making (cite). Prior tasks that activate frontal cortex and dorsal striatum should foster model-based decision making, whereas tasks that activate subcortical regions and ventral stiatum should increase model-free decisions. Lastly, tasks that are unrelated to model-based and model-free processes or brain areas should have no influence on subsequent decision making.

Preregistered on osf.

\section{Methods and Results}
\subsection{Study Design}

Based on these assumptions, we designed the following cognitive intervention: 125(???) Participants came in for two sessions, two days apart. Participants' decision strategies were assessed using the 2-step task at the beginning and end of each session, i.e., four times in total. Participants were randomly assigned to one of five groups. The control group engaged in the same two control tasks during the first and second session, a number comparison task, and a orientation-discrimination task. The model-based group (MB-MB) received the Tower of London task (cite) and a rule-based category task (Ashby \& Maddox) during both sessions and the model-free group (MF-MF) received a simple operant condition task (cite) and an information-integration category task in both sessions. The last two groups of participants were mixed: the MB-MF group received the tasks of the MB-MB group in the first session, and the tasks of the MF-MF group in the second. The opposite was true for the MF-MB group. We hoped to see the effects of the first session reversed after the second session in the mixed groups.

Excluded participants and trials?

We used the much-used 2-step decision task to assess model-based and model-free behavior. In each trial of this task, participants first select one of two stimuli (e.g., blue vs green). Depending on the selected stimulus (e.g., green), participants are presented with a second binary decision (e.g., red vs yellow). The second choice determines whether participants will be rewarded or not. Each stimulus in the second stage is associated with a certain, independent reward probability. Crucially, the choice in the first stage only probabilistically determines which pair of stimuli will be presented in the second stage. In the example, selecting the green stimulus in the first stage leads to the read-vs-yellow stage in 70\% of cases, but to the pink-vs-purple stage in 30\% of the trials. The opposite is true when the blue stimulus is selected in the first stage. A model-free agent learns the values of each first-stage stimulus by averaging the rewards obtained after selecting this stimulus. The model-free agent does therefore not discriminate between common and rare transitions between the first and second stage. A model-based agent, on the other hand, selects actions based on mental simulations, taking into account its knowledge about task contingencies, including common and rare transitions. 

\subsection{Behavioral Switch-Stay Analysis}
Participants' decision making strategies in the 2-step task can be analyzed in two ways: Using behavioral methods, it can be tested whether participants differentiate learn differently from common and rare state transitions. Using RL modeling, model-based and model-free learning can be modeled explicitly. These models can then be fit to the human data.

Using the first approach, we first assessed participants' tendencies to repeat first-stage decisions after receiving a reward at the end of the trial. A model-free agent would repeat a rewarded action irrespective of whether it was followed by a common or rare transition. A model-based agent, on the other hand, would consider picking to other first-stage action after a rewarded rare transition because the probability of achieving the same outcome is higher after picking the opposite first-stage action. Figure 1 shows that all five groups of participants showed similar patterns of staying with the previously selected action or switching to the opposite action. Participants repeated rewarded actions more often than unrewarded actions. This is also evident in the significant main effect of reward on staying in the next trial (Figure 2), obtained through logistic, mixed-effects regression models using the package nler4 (cite) in R (cite). In addition to this sensitivity to reward, a feature of model-based and model-free decision strategies alike, participants also showed some sensitivity to the transition, which is characteristic of model-based decision making. Participants more often stayed with rewarded actions and more often switched away from unrewarded actions after common transitions. This was also reflected in an interaction between reward and transition in the regression models (Figure 2).

Participants did not differ between groups in terms of decision strategies. There were no interactions between reward and group, suggesting a difference in reward sensitivity, or between reward, transition, and group, suggesting a difference in the model-based aspect. 

\subsection{RL Modeling Analysis}
We next used RL models to assess participant behavior. In the models, agents learned values for each of the six actions in the game (i.e., the six stimuli) and used these values to select actions, as follows:

\begin{equation}
P(a|s) = \frac{exp(\beta_{1} Q(s,a)}{\sum_{i} exp(\beta_{1} Q(s,a_{i}) + k K + p P)}
\end{equation}

Beta is the agent's individual softmax temperature, which regulates whether the agent will always choose the option with the higher value, or explore both actions more equally. Different values were allowed to select the first and second stimulus (beta1 and beta2). Explain keyrep and fracrep.

In order to learn action values, agents observe the outcome of each trial (reward R = 1 or R = 0) and subsequently update the value V of the selected second-stage stimulus, in the following way:

\begin{equation}
Q(s_{t+1}, a_{t+1}) = Q(s_{t}, a_{t}) + \alpha_{2} RPE_{t},
\end{equation}
\begin{equation}
RPE_{t} = r_{t} - Q(s_{t}, a_{t})
\end{equation}

where $ \alpha_{2} $ is an agent's second-stage learning rate.

Having updated the value of the second-stage stimulus, agents then update the values of first-stage actions. Model-free agents update the value of the selected first-stage action as follows:

\begin{equation}
Q(s_{t+1}, a_{t+1}) = Q(s_{t}, a_{t}) + \alpha_{1} * (VPE_{t} + \lambda RPE_{t})
\end{equation}
\begin{equation}
VPE_{t} = Q2 - Q1
\end{equation}

The value of the first-stage stimulus depends on the reward prediction error (RPE) and a value prediction error (VPE), the difference between the value of the first and second selected action. lambda is a discounting factor for the RPE (the RPE only occurs after the second decision has been made).

The model-based agent, on the other hand, determines the values of first-stage action based on its internal model:

\begin{equation}
V1a(t) = common * max(V2a, V2b) + (1 - common) * max(V2c, V2d) 
V1b(t) = (1 - common) * max(V2a, V2b) + common * max(V2c, V2d),
\end{equation}

where V2a and V2b is the pair of second-stage stimuli that usually follows the selection of the first-stage stimulus V1a; V2c and V2d are the second-stage pair that usually follows V1b. common is the probability of the common transition.

Participants in this task were modeled as hybrid agents, combining model-based and model-free value estimates:

\begin{equation}
Vhyb(t) = (1 - w) * Vmf + w * Vmb
\end{equation}

The parameter w therefore determines how much action values---and therefore actions---are determined by model-based and model-free value learning.

We validated our model by simulating agents with different parameters, and subsequently recovering these parameters (data not shown). We also confirmed that reduced models, with certain parameters fixed, were best recovered by models with the same free parameters, rather than different models. 

Having confirmed the validity of our model, we next aimed to characterize human performance in terms of the free parameters of the model---learning rates alpha1 and alpha2, decision temperature beta1 and beta2, future discounting lambda, key perseverance k, choice perseverance p, and balance between model-based and model-free decision making w. To this aim, we first determined if a simplified model fit the behavioral data better than the full model. Indeed, the Bayesian Information Criterion (BIC) was lower for a reduced model with only 5 parameters---keeping key perseverance at k = 0 and discounting lambda at 0.

We then fit the simplified model to individual participants' behavior. To this aim, we selected the free parameters such that the likelihood of the participant's responses was maximized. We used Matlab's fmincon function with 30 different start values to find the best-fitting parameter values for each participant, minimizing the negative log of the likelihood. Each participant's data were thereby associated with a specific set of parameters, characterizing this participant's behavior. The results are shown Figure 3. We found that, contrary to our expectations but in line with the behavioral analyses, different training tasks did not lead to differences in w, the balance between model-based and model-free decisions. 

On an individual subject level, we found changes between assessments, but nothing consistent for the whole group.

\section{Conclusion}
We present the results of a study aimed at investigating factors that influence human decision making. In specific, we aimed to test whether the use of model-based versus model-free strategies depended on previous cognitive activities, such that engagement in mental simulation and forward planning would facilitate model-based decision strategies, whereas habitual, reward-driven, stimulus-response behavior would promote model-free decision making. We selected tasks with these cognitive components that had also been (excessively) studied using functional magnetic resonance imaging (fMRI), so that we can conclude that the model-based set of tasks (ToL, rb category) engaged brain regions that have been associated with model-based decision making, whereas the model-free tasks engaged brain regions associated with model-free decision making. 

Our study revealed no influence of the cognitive intervention tasks on decision making. This can not be attributed to limited power because the samples size (25 participants per group) had been determined based on a power analysis in a pilot sample, which had shown promising results. 

Instead, the results suggest that previous cognitive activities do not influence subsequent decision making---at least at the scale of the duration of the 2-step task (approximately 15 minutes???). It is possible that carry-over effects from previous tasks exist on a shorter time scale, but we are not aware of a shorter test of decision making strategy than the used 2-step task. Lastly, the discrepancy between the pilot data, in which there were hints of an effect, and the current study might also arise from the fact that participants did the 2-step task 4 versus 2 times. Maybe it only works when you have never seen it before.

Concluding, research into the manipulation of model-based and model-free decision strategies is of great relevance. Model-free decisions are often short-sighted and suboptimal on the long term (the chocolate cake). Model-based decisions require more effort but can lead to better long-term outcomes. Many vulnerable subjects, including persons with ADHD, depression, schizophrenia, Parkinson's disease, obesity, impulsitiy, eating disorders, or addiction would benefit from a cognitive intervention that facilitates model-based decision making and creates some ease in selecting and switching between both strategies, according to the context.

\section{References}


\end{document}
